{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao83N5ObYxV2",
        "outputId": "f7544e07-4d7e-4574-cead-de492450bc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pojmAEMKaHn6",
        "outputId": "feaa0e80-3b43-436d-a728-15d6975510ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/data.zip\n",
            "  inflating: HR_all.pkl              \n",
            "  inflating: label_all.pkl           \n",
            "  inflating: OX_all.pkl              \n",
            "  inflating: SaO2_all.pkl            \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iQVvDsVxajpy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset , random_split , DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SWR8H9_ibKVG"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WBKHgA-MbKrs"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EmbeddingEncoder:\n",
        "    def __call__(\n",
        "        self,\n",
        "        data : torch.Tensor ,\n",
        "        format : str = \"sum\",\n",
        "        stride : int = 2 ,\n",
        "        padding_format : str = \"concat\"\n",
        "    ) -> torch.Tensor :\n",
        "        # {\n",
        "        #    data : [ batch , max_length ]\n",
        "        # }\n",
        "\n",
        "        encoder_all = torch.zeros(data.size() , dtype=torch.float)\n",
        "\n",
        "        for i in range(data.size(1)):\n",
        "\n",
        "            _ = [\n",
        "                data[: , i : i + stride ]\n",
        "            ]\n",
        "\n",
        "            if _[0].size(1) < stride :\n",
        "                _.append(data[: , 0 : stride - _[0].size(1)])\n",
        "\n",
        "            if format == \"tanh\": # => stride => 10\n",
        "                encoder_all[: , i] = torch.cat(_ , dim=-1).float().tanh().mean(dim=-1)\n",
        "\n",
        "            elif format == \"sinh\": # => stride => 10\n",
        "                encoder_all[: , i] = torch.cat(_ , dim=-1).float().sinh().mean(dim=-1)\n",
        "\n",
        "            elif format == \"softmax\": # => stride => 2\n",
        "                encoder_all[: , i] = torch.cat(_ , dim=-1).float().softmax(dim=-1).sum(dim=-1).softmax(dim=-1)\n",
        "\n",
        "        return encoder_all\n",
        "\n",
        "class Datasets(Dataset):\n",
        "    def __init__(self ,\n",
        "        datasets_HR : pd.DataFrame ,\n",
        "        datasets_OX : pd.DataFrame ,\n",
        "        datasets_SaO2 : pd.DataFrame ,\n",
        "        datasets_labels : pd.DataFrame\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._HR , self._OX , self._SaO2 , self._labels = datasets_HR , datasets_OX , datasets_SaO2 , datasets_labels\n",
        "\n",
        "        self._ModelEncoder = EmbeddingEncoder()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._HR)\n",
        "\n",
        "    def __getitem__(self, index : int ):\n",
        "\n",
        "        return {\n",
        "            \"HR\" : torch.from_numpy(self._HR.iloc[index].values).float().flatten() ,\n",
        "            \"OX\" : torch.from_numpy(self._OX.iloc[index].values).float().flatten() ,\n",
        "            \"SaO2\" : torch.from_numpy(self._SaO2.iloc[index].values).float().flatten() ,\n",
        "            \"labels\" : torch.from_numpy(self._labels.iloc[index].values).float().flatten() ,\n",
        "        }\n",
        "\n",
        "data = Datasets(\n",
        "    datasets_HR= pd.read_pickle('/content/HR_all.pkl')  ,\n",
        "    datasets_OX= pd.read_pickle('/content/OX_all.pkl') ,\n",
        "    datasets_SaO2= pd.read_pickle('/content/SaO2_all.pkl') ,\n",
        "    datasets_labels= pd.read_pickle('/content/label_all.pkl')\n",
        ")\n",
        "\n",
        "train_dataloder =  DataLoader(data, batch_size= 50 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bISNYbxGbL9E"
      },
      "outputs": [],
      "source": [
        "class SpatialAttentionBlock(nn.Module):\n",
        "    def __init__(self , in_channel : int = 1024):\n",
        "        super().__init__()\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_channel, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self , input_ids : torch.Tensor ):\n",
        "        return input_ids * self._model(input_ids)\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self,  new_channels : int , last_channels : int):\n",
        "        super().__init__()\n",
        "\n",
        "        self._avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self._attn = nn.Sequential(\n",
        "            nn.Linear(new_channels , last_channels , bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(last_channels , new_channels , bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids : torch.Tensor ):\n",
        "\n",
        "        _avg = self._avgpool(input_ids)\n",
        "\n",
        "        return input_ids * self._attn(_avg.flatten(1)).view_as(_avg)\n",
        "\n",
        "class DualAttention(nn.Module):\n",
        "    def __init__(self, new_channels : int , last_channels : int ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_attn = ChannelAttention(new_channels=new_channels , last_channels=last_channels)\n",
        "        self.spatial_attn = SpatialAttentionBlock(in_channel=new_channels)\n",
        "\n",
        "    def forward(self, input_ids : torch.Tensor):\n",
        "        return  self.spatial_attn(\n",
        "            self.channel_attn(\n",
        "                input_ids\n",
        "            )\n",
        "        )\n",
        "\n",
        "class BlockEncoderCnn(nn.Module):\n",
        "    def __init__(self , in_channels , out_channels , kernel_size : int = 3 , stride : int = 1 , padding : int = 1 , level : int = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        if level == 0 :\n",
        "\n",
        "            self._model = nn.Sequential(\n",
        "                nn.Conv1d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size , stride=stride , padding=padding , bias=False ),\n",
        "                nn.BatchNorm1d( num_features=out_channels ),\n",
        "                nn.LeakyReLU( inplace = True ),\n",
        "                nn.Conv1d( out_channels, out_channels, kernel_size=1 ) ,\n",
        "                DualAttention( new_channels= out_channels , last_channels= in_channels )\n",
        "            )\n",
        "\n",
        "            self._activation = nn.Sequential(\n",
        "                nn.BatchNorm1d(num_features=out_channels) ,\n",
        "                nn.ReLU() ,\n",
        "                nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "            )\n",
        "\n",
        "        elif level == 1 :\n",
        "\n",
        "            self._model = nn.Sequential(\n",
        "                nn.Conv1d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size , stride=stride , padding=padding , bias=False ),\n",
        "                nn.BatchNorm1d( num_features=out_channels ),\n",
        "                nn.LeakyReLU( inplace = True )\n",
        "            )\n",
        "\n",
        "            self._activation = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        else :\n",
        "\n",
        "            self._model = nn.Sequential(\n",
        "                nn.Conv1d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size , stride=stride , padding=padding , bias=False ),\n",
        "                DualAttention( new_channels= out_channels , last_channels= in_channels )\n",
        "            )\n",
        "\n",
        "            self._activation = nn.Sequential(\n",
        "                nn.BatchNorm1d(num_features=out_channels) ,\n",
        "                nn.ReLU() ,\n",
        "                nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "            )\n",
        "\n",
        "    def forward(self , input_ids : torch.Tensor ):\n",
        "        return self._activation(self._model(input_ids))\n",
        "\n",
        "class EncoderCnnExtra(nn.Module):\n",
        "    def __init__(self, level : int = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        3   , 256 , 256\n",
        "        32  , 128 , 128\n",
        "        64  , 64  , 64\n",
        "        128 , 32  , 32\n",
        "        256 , 16  , 16\n",
        "        512 , 8   , 8\n",
        "        1024 , 4  , 4\n",
        "        2048 , 2  , 2\n",
        "        \"\"\"\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            BlockEncoderCnn( in_channels=1   , out_channels=32   , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            BlockEncoderCnn( in_channels=32  , out_channels=64   , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            BlockEncoderCnn( in_channels=64  , out_channels=128  , kernel_size=15 , stride=2 , padding=2 , level=level),\n",
        "            BlockEncoderCnn( in_channels=128 , out_channels=256  , kernel_size=15 , stride=2 , padding=2 , level=level),\n",
        "            BlockEncoderCnn( in_channels=256 , out_channels=512  , kernel_size=11 , stride=2 , padding=2 , level=level),\n",
        "            BlockEncoderCnn( in_channels=512 , out_channels=1024 , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            BlockEncoderCnn( in_channels=1024 ,out_channels=2048 , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "        )\n",
        "\n",
        "    def forward(self , input_ids : torch.Tensor ):\n",
        "        return self._model(input_ids.unsqueeze(1)).permute(0 , 2 , 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6Dn7jZMpcDp7"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self , level : int = 0) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._cnn = EncoderCnnExtra(level)\n",
        "\n",
        "        self._dropout = nn.Dropout(0.4)\n",
        "\n",
        "        self._model = nn.Linear(2048 , 4)\n",
        "\n",
        "    def forward(self , x : torch.Tensor ):\n",
        "\n",
        "        return self._model(self._dropout(self._cnn(x))).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zuGrmiT_cFqr"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        inputs: logits با shape [batch_size, num_classes]\n",
        "        targets: اندیس کلاس صحیح با shape [batch_size]\n",
        "        \"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u0mXFDAwcHoa"
      },
      "outputs": [],
      "source": [
        "level = 0\n",
        "model = Model(level).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters() , lr=0.00001 ) # lr/epoch\n",
        "loss_func = FocalLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_TGKcNqcI2K",
        "outputId": "49cef746-d721-4657-ad92-e359988ceadc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.3132)\n",
            "epoch 0 train_loss =>  0.7821266100520179 acc Train =>  tensor(0.3132)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:33<00:00,  3.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.3786)\n",
            "epoch 1 train_loss =>  0.709903952053615 acc Train =>  tensor(0.3786)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:34<00:00,  3.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.4221)\n",
            "epoch 2 train_loss =>  0.660879447346642 acc Train =>  tensor(0.4221)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:35<00:00,  2.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.4569)\n",
            "epoch 3 train_loss =>  0.6243083661510831 acc Train =>  tensor(0.4569)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:35<00:00,  2.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.5177)\n",
            "epoch 4 train_loss =>  0.5766573494388944 acc Train =>  tensor(0.5177)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.5907)\n",
            "epoch 5 train_loss =>  0.5105611060346876 acc Train =>  tensor(0.5907)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.6649)\n",
            "epoch 6 train_loss =>  0.43942512784685406 acc Train =>  tensor(0.6649)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.7525)\n",
            "epoch 7 train_loss =>  0.3449726973261152 acc Train =>  tensor(0.7525)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.8301)\n",
            "epoch 8 train_loss =>  0.257892050913402 acc Train =>  tensor(0.8301)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.8983)\n",
            "epoch 9 train_loss =>  0.17300128564238548 acc Train =>  tensor(0.8983)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.9371)\n",
            "epoch 10 train_loss =>  0.11295241106833731 acc Train =>  tensor(0.9371)\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "_lossed = []\n",
        "_acc_train = []\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    _list = []\n",
        "    acc_train = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for data in tqdm(train_dataloder):\n",
        "\n",
        "        # 0 , 1 , 2\n",
        "        # data[\"HR\"].to(device) , data[\"OX\"].to(device) , data[\"SaO2\"].to(device)\n",
        "\n",
        "        predictions = model(data[\"HR\"].to(device))\n",
        "\n",
        "        loss = loss_func(predictions, data[\"labels\"].squeeze(1).long().to(device))\n",
        "\n",
        "        predict_classes_val = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        acc_train.append(torch.sum(predict_classes_val.cpu() == data[\"labels\"].squeeze(1).long()) / predict_classes_val.size(0))\n",
        "\n",
        "        _list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        _ = nn.utils.clip_grad_norm_(model.parameters(), 50.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    if sum(acc_train) / len(acc_train) > best_acc:\n",
        "\n",
        "        torch.save({\n",
        "            \"model\": model._cnn.state_dict()\n",
        "        } , f\"./data_best_{level}_HR.pt\")\n",
        "\n",
        "        best_acc = sum(acc_train) / len(acc_train)\n",
        "\n",
        "        print(\"best acc test \" , best_acc)\n",
        "\n",
        "    print(\"epoch\", epoch , \"train_loss => \" , sum(_list) / len(_list) , \"acc Train => \" , sum(acc_train) / len(acc_train) )\n",
        "\n",
        "    _lossed.append(_list)\n",
        "    _acc_train.append(acc_train)\n",
        "\n",
        "    if sum(_list) / len(_list) < 0.15 :\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syDx-PkJcN2r",
        "outputId": "e4a17c09-f118-4de2-9c06-5401b33718a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:34<00:00,  3.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.3391)\n",
            "epoch 0 train_loss =>  0.8362609420503889 acc Train =>  tensor(0.3391)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:35<00:00,  2.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.5574)\n",
            "epoch 1 train_loss =>  0.5252310514450074 acc Train =>  tensor(0.5574)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:35<00:00,  2.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.7598)\n",
            "epoch 2 train_loss =>  0.30923901817628313 acc Train =>  tensor(0.7598)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:35<00:00,  2.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.9057)\n",
            "epoch 3 train_loss =>  0.15087763862240883 acc Train =>  tensor(0.9057)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.9663)\n",
            "epoch 4 train_loss =>  0.07000280977005051 acc Train =>  tensor(0.9663)\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "_lossed = []\n",
        "_acc_train = []\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    _list = []\n",
        "    acc_train = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for data in tqdm(train_dataloder):\n",
        "\n",
        "        # 0 , 1 , 2\n",
        "        # data[\"HR\"].to(device) , data[\"OX\"].to(device) , data[\"SaO2\"].to(device)\n",
        "\n",
        "        predictions = model(data[\"OX\"].to(device))\n",
        "\n",
        "        loss = loss_func(predictions, data[\"labels\"].squeeze(1).long().to(device))\n",
        "\n",
        "        predict_classes_val = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        acc_train.append(torch.sum(predict_classes_val.cpu() == data[\"labels\"].squeeze(1).long()) / predict_classes_val.size(0))\n",
        "\n",
        "        _list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        _ = nn.utils.clip_grad_norm_(model.parameters(), 50.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    if sum(acc_train) / len(acc_train) > best_acc:\n",
        "\n",
        "        torch.save({\n",
        "            \"model\": model._cnn.state_dict()\n",
        "        } , f\"./data_best_{level}_OX.pt\")\n",
        "\n",
        "        best_acc = sum(acc_train) / len(acc_train)\n",
        "\n",
        "        print(\"best acc test \" , best_acc)\n",
        "\n",
        "    print(\"epoch\", epoch , \"train_loss => \" , sum(_list) / len(_list) , \"acc Train => \" , sum(acc_train) / len(acc_train) )\n",
        "\n",
        "    _lossed.append(_list)\n",
        "    _acc_train.append(acc_train)\n",
        "\n",
        "    if sum(_list) / len(_list) < 0.15 :\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8uigLPjcjNz",
        "outputId": "26c28656-3e43-402a-affa-939c332a693b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:35<00:00,  2.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.5955)\n",
            "epoch 0 train_loss =>  0.454664951137134 acc Train =>  tensor(0.5955)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.8061)\n",
            "epoch 1 train_loss =>  0.18890386553747313 acc Train =>  tensor(0.8061)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:36<00:00,  2.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best acc test  tensor(0.8960)\n",
            "epoch 2 train_loss =>  0.10457750072791464 acc Train =>  tensor(0.8960)\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "_lossed = []\n",
        "_acc_train = []\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "\n",
        "    _list = []\n",
        "    acc_train = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for data in tqdm(train_dataloder):\n",
        "\n",
        "        # 0 , 1 , 2\n",
        "        # data[\"HR\"].to(device) , data[\"OX\"].to(device) , data[\"SaO2\"].to(device)\n",
        "\n",
        "        predictions = model(data[\"SaO2\"].to(device))\n",
        "\n",
        "        loss = loss_func(predictions, data[\"labels\"].squeeze(1).long().to(device))\n",
        "\n",
        "        predict_classes_val = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        acc_train.append(torch.sum(predict_classes_val.cpu() == data[\"labels\"].squeeze(1).long()) / predict_classes_val.size(0))\n",
        "\n",
        "        _list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        _ = nn.utils.clip_grad_norm_(model.parameters(), 50.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    if sum(acc_train) / len(acc_train) > best_acc:\n",
        "\n",
        "        torch.save({\n",
        "            \"model\": model._cnn.state_dict()\n",
        "        } , f\"./data_best_{level}_SaO2.pt\")\n",
        "\n",
        "        best_acc = sum(acc_train) / len(acc_train)\n",
        "\n",
        "        print(\"best acc test \" , best_acc)\n",
        "\n",
        "    print(\"epoch\", epoch , \"train_loss => \" , sum(_list) / len(_list) , \"acc Train => \" , sum(acc_train) / len(acc_train) )\n",
        "\n",
        "    _lossed.append(_list)\n",
        "    _acc_train.append(acc_train)\n",
        "\n",
        "    if sum(_list) / len(_list) < 0.15 :\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5MZxjxNGfSCv"
      },
      "outputs": [],
      "source": [
        "class IndexingData:\n",
        "    def __call__(self , data_labels : pd.DataFrame ):\n",
        "        index = {\n",
        "            0 : [] ,\n",
        "            1 : [] ,\n",
        "            2 : [] ,\n",
        "            3 : [] ,\n",
        "        }\n",
        "\n",
        "        for i in range(len(data_labels)):\n",
        "\n",
        "            label = int(data_labels.iloc[i].label)\n",
        "\n",
        "            index[label].append(i)\n",
        "\n",
        "        minimum = min([len(index[0]) , len(index[1]) , len(index[2]) , len(index[3]) ])\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for _class in index:\n",
        "\n",
        "            labels = index[_class]\n",
        "\n",
        "            random.shuffle(labels)\n",
        "\n",
        "            data = data + labels[0:minimum]\n",
        "\n",
        "        return data\n",
        "\n",
        "class EmbeddingEncoder:\n",
        "    def __call__(\n",
        "        self,\n",
        "        data : torch.Tensor ,\n",
        "        format : str = \"sum\",\n",
        "        stride : int = 2 ,\n",
        "        padding_format : str = \"concat\"\n",
        "    ) -> torch.Tensor :\n",
        "        # {\n",
        "        #    data : [ batch , max_length ]\n",
        "        # }\n",
        "\n",
        "        encoder_all = torch.zeros(data.size() , dtype=torch.float)\n",
        "\n",
        "        for i in range(data.size(1)):\n",
        "\n",
        "            _ = [\n",
        "                data[: , i : i + stride ]\n",
        "            ]\n",
        "\n",
        "            if _[0].size(1) < stride :\n",
        "                _.append(data[: , 0 : stride - _[0].size(1)])\n",
        "\n",
        "            if format == \"tanh\": # => stride => 10\n",
        "                encoder_all[: , i] = torch.cat(_ , dim=-1).float().tanh().mean(dim=-1)\n",
        "\n",
        "            elif format == \"sinh\": # => stride => 10\n",
        "                encoder_all[: , i] = torch.cat(_ , dim=-1).float().sinh().mean(dim=-1)\n",
        "\n",
        "            elif format == \"softmax\": # => stride => 2\n",
        "                encoder_all[: , i] = torch.cat(_ , dim=-1).float().softmax(dim=-1).sum(dim=-1).softmax(dim=-1)\n",
        "\n",
        "        return encoder_all\n",
        "\n",
        "class Datasets(Dataset):\n",
        "    def __init__(self ,\n",
        "        datasets_HR : pd.DataFrame ,\n",
        "        datasets_OX : pd.DataFrame ,\n",
        "        datasets_SaO2 : pd.DataFrame ,\n",
        "        datasets_labels : pd.DataFrame ,\n",
        "        indexing : list ,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._HR , self._OX , self._SaO2 , self._labels = datasets_HR , datasets_OX , datasets_SaO2 , datasets_labels\n",
        "\n",
        "        self._indexing = indexing\n",
        "\n",
        "        self._ModelEncoder = EmbeddingEncoder()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._indexing)\n",
        "\n",
        "    def __getitem__(self, index : int ):\n",
        "\n",
        "        index = self._indexing[index]\n",
        "\n",
        "        return {\n",
        "            \"HR\" : torch.from_numpy(self._HR.iloc[index].values).float().flatten() ,\n",
        "            \"OX\" : torch.from_numpy(self._OX.iloc[index].values).float().flatten() ,\n",
        "            \"SaO2\" : torch.from_numpy(self._SaO2.iloc[index].values).float().flatten() ,\n",
        "            \"labels\" : torch.from_numpy(self._labels.iloc[index].values).float().flatten() ,\n",
        "        }\n",
        "\n",
        "datasets_labels = pd.read_pickle('/content/label_all.pkl')\n",
        "indexing = IndexingData()\n",
        "\n",
        "data = Datasets(\n",
        "    datasets_HR= pd.read_pickle('/content/HR_all.pkl')  ,\n",
        "    datasets_OX= pd.read_pickle('/content/OX_all.pkl') ,\n",
        "    datasets_SaO2= pd.read_pickle('/content/SaO2_all.pkl') ,\n",
        "    datasets_labels= datasets_labels ,\n",
        "    indexing=indexing(datasets_labels.rename(columns={0 : 'label'}))\n",
        ")\n",
        "\n",
        "train_data, valid_data = random_split(data, [0.8 , 0.2] , generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_dataloder =  DataLoader(train_data, batch_size= 32 , shuffle= True)\n",
        "valid_dataloder =  DataLoader(valid_data, batch_size= 32 , shuffle= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ee4y9Xx8fR_6"
      },
      "outputs": [],
      "source": [
        "class SpatialAttentionBlock(nn.Module):\n",
        "    def __init__(self , in_channel : int = 1024):\n",
        "        super().__init__()\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_channel, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self , input_ids : torch.Tensor ):\n",
        "        return input_ids * self._model(input_ids)\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self,  new_channels : int , last_channels : int):\n",
        "        super().__init__()\n",
        "\n",
        "        self._avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self._attn = nn.Sequential(\n",
        "            nn.Linear(new_channels , last_channels , bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(last_channels , new_channels , bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids : torch.Tensor ):\n",
        "\n",
        "        _avg = self._avgpool(input_ids)\n",
        "\n",
        "        return input_ids * self._attn(_avg.flatten(1)).view_as(_avg)\n",
        "\n",
        "class DualAttention(nn.Module):\n",
        "    def __init__(self, new_channels : int , last_channels : int ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_attn = ChannelAttention(new_channels=new_channels , last_channels=last_channels)\n",
        "        self.spatial_attn = SpatialAttentionBlock(in_channel=new_channels)\n",
        "\n",
        "    def forward(self, input_ids : torch.Tensor):\n",
        "        return  self.spatial_attn(\n",
        "            self.channel_attn(\n",
        "                input_ids\n",
        "            )\n",
        "        )\n",
        "\n",
        "class BlockEncoderCnn(nn.Module):\n",
        "    def __init__(self , in_channels , out_channels , kernel_size : int = 3 , stride : int = 1 , padding : int = 1 , level : int = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        if level == 0 :\n",
        "            self._model = nn.Sequential(\n",
        "                nn.Conv1d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size , stride=stride , padding=padding , bias=False ),\n",
        "                nn.BatchNorm1d( num_features=out_channels ),\n",
        "                nn.LeakyReLU( inplace = True ),\n",
        "                nn.Conv1d( out_channels, out_channels, kernel_size=1 ) ,\n",
        "                DualAttention( new_channels= out_channels , last_channels= in_channels )\n",
        "            )\n",
        "\n",
        "            self._activation = nn.Sequential(\n",
        "                nn.BatchNorm1d(num_features=out_channels) ,\n",
        "                nn.ReLU() ,\n",
        "                nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "            )\n",
        "\n",
        "        elif level == 1 :\n",
        "\n",
        "            self._model = nn.Sequential(\n",
        "                nn.Conv1d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size , stride=stride , padding=padding , bias=False ),\n",
        "                nn.BatchNorm1d( num_features=out_channels ),\n",
        "                nn.LeakyReLU( inplace = True )\n",
        "            )\n",
        "\n",
        "            self._activation = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        else :\n",
        "\n",
        "            self._model = nn.Sequential(\n",
        "                nn.Conv1d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size , stride=stride , padding=padding , bias=False ),\n",
        "                DualAttention( new_channels= out_channels , last_channels= in_channels )\n",
        "            )\n",
        "\n",
        "            self._activation = nn.Sequential(\n",
        "                nn.BatchNorm1d(num_features=out_channels) ,\n",
        "                nn.ReLU() ,\n",
        "                nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "            )\n",
        "\n",
        "    def forward(self , input_ids : torch.Tensor ):\n",
        "        return self._activation(self._model(input_ids))\n",
        "\n",
        "class EncoderCnnExtra(nn.Module):\n",
        "    def __init__(self, level : int = 0):\n",
        "        super().__init__()\n",
        "        self._model = nn.Sequential(\n",
        "            BlockEncoderCnn( in_channels=1   , out_channels=32   , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            BlockEncoderCnn( in_channels=32  , out_channels=64   , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            BlockEncoderCnn( in_channels=64  , out_channels=128  , kernel_size=15 , stride=2 , padding=2 , level=level),\n",
        "            BlockEncoderCnn( in_channels=128 , out_channels=256  , kernel_size=15 , stride=2 , padding=2 , level=level),\n",
        "            BlockEncoderCnn( in_channels=256 , out_channels=512  , kernel_size=11 , stride=2 , padding=2 , level=level),\n",
        "            BlockEncoderCnn( in_channels=512 , out_channels=1024 , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            BlockEncoderCnn( in_channels=1024 ,out_channels=2048 , kernel_size=3  , stride=1 , padding=1 , level=level),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "        )\n",
        "\n",
        "    def forward(self , input_ids : torch.Tensor ):\n",
        "        return self._model(input_ids.unsqueeze(1)).permute(0 , 2 , 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RAGnuoXOfR9K"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__( self , num_class : int = 4 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self._cnn = nn.ModuleList([EncoderCnnExtra(level=level) for level in range(3)])\n",
        "\n",
        "        self._embedding_position = nn.ModuleList([nn.LSTM(2048 , 256 , batch_first=True ) for _ in range(3)])\n",
        "\n",
        "        self._attn = nn.MultiheadAttention(embed_dim=256 * 3 , num_heads=3 , dropout=0.2 , batch_first=True )\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            nn.Linear(256 * 3 , num_class) ,\n",
        "            nn.Dropout(0.1) ,\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self ,\n",
        "        HR : torch.Tensor ,\n",
        "        OX : torch.Tensor ,\n",
        "        saO2 : torch.Tensor\n",
        "    ):\n",
        "        hidden = []\n",
        "\n",
        "        for i , data in enumerate([HR , OX , saO2]):\n",
        "\n",
        "            hidden.append(\n",
        "                self._embedding_position[i](\n",
        "                    self._cnn[i](data)\n",
        "                )[0]\n",
        "            )\n",
        "\n",
        "        hidden = torch.cat(hidden , dim=-1).squeeze(1)\n",
        "\n",
        "        return self._model(\n",
        "            self._attn(\n",
        "                hidden , hidden , hidden\n",
        "            )[0]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O7e4_swhfa3G"
      },
      "outputs": [],
      "source": [
        "model = Model(num_class=4).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8P0bT_Dfc59",
        "outputId": "8144a6cf-5890-4dd7-ab0c-498a8490cddf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<All keys matched successfully>,\n",
              " _IncompatibleKeys(missing_keys=[], unexpected_keys=['_model.0._model.3.weight', '_model.0._model.3.bias', '_model.0._model.4.channel_attn._attn.0.weight', '_model.0._model.4.channel_attn._attn.2.weight', '_model.0._model.4.spatial_attn._model.0.weight', '_model.0._activation.0.weight', '_model.0._activation.0.bias', '_model.0._activation.0.running_mean', '_model.0._activation.0.running_var', '_model.0._activation.0.num_batches_tracked', '_model.1._model.3.weight', '_model.1._model.3.bias', '_model.1._model.4.channel_attn._attn.0.weight', '_model.1._model.4.channel_attn._attn.2.weight', '_model.1._model.4.spatial_attn._model.0.weight', '_model.1._activation.0.weight', '_model.1._activation.0.bias', '_model.1._activation.0.running_mean', '_model.1._activation.0.running_var', '_model.1._activation.0.num_batches_tracked', '_model.2._model.3.weight', '_model.2._model.3.bias', '_model.2._model.4.channel_attn._attn.0.weight', '_model.2._model.4.channel_attn._attn.2.weight', '_model.2._model.4.spatial_attn._model.0.weight', '_model.2._activation.0.weight', '_model.2._activation.0.bias', '_model.2._activation.0.running_mean', '_model.2._activation.0.running_var', '_model.2._activation.0.num_batches_tracked', '_model.3._model.3.weight', '_model.3._model.3.bias', '_model.3._model.4.channel_attn._attn.0.weight', '_model.3._model.4.channel_attn._attn.2.weight', '_model.3._model.4.spatial_attn._model.0.weight', '_model.3._activation.0.weight', '_model.3._activation.0.bias', '_model.3._activation.0.running_mean', '_model.3._activation.0.running_var', '_model.3._activation.0.num_batches_tracked', '_model.4._model.3.weight', '_model.4._model.3.bias', '_model.4._model.4.channel_attn._attn.0.weight', '_model.4._model.4.channel_attn._attn.2.weight', '_model.4._model.4.spatial_attn._model.0.weight', '_model.4._activation.0.weight', '_model.4._activation.0.bias', '_model.4._activation.0.running_mean', '_model.4._activation.0.running_var', '_model.4._activation.0.num_batches_tracked', '_model.5._model.3.weight', '_model.5._model.3.bias', '_model.5._model.4.channel_attn._attn.0.weight', '_model.5._model.4.channel_attn._attn.2.weight', '_model.5._model.4.spatial_attn._model.0.weight', '_model.5._activation.0.weight', '_model.5._activation.0.bias', '_model.5._activation.0.running_mean', '_model.5._activation.0.running_var', '_model.5._activation.0.num_batches_tracked', '_model.6._model.3.weight', '_model.6._model.3.bias', '_model.6._model.4.channel_attn._attn.0.weight', '_model.6._model.4.channel_attn._attn.2.weight', '_model.6._model.4.spatial_attn._model.0.weight', '_model.6._activation.0.weight', '_model.6._activation.0.bias', '_model.6._activation.0.running_mean', '_model.6._activation.0.running_var', '_model.6._activation.0.num_batches_tracked']),\n",
              " _IncompatibleKeys(missing_keys=['_model.0._model.1.channel_attn._attn.0.weight', '_model.0._model.1.channel_attn._attn.2.weight', '_model.0._model.1.spatial_attn._model.0.weight', '_model.1._model.1.channel_attn._attn.0.weight', '_model.1._model.1.channel_attn._attn.2.weight', '_model.1._model.1.spatial_attn._model.0.weight', '_model.2._model.1.channel_attn._attn.0.weight', '_model.2._model.1.channel_attn._attn.2.weight', '_model.2._model.1.spatial_attn._model.0.weight', '_model.3._model.1.channel_attn._attn.0.weight', '_model.3._model.1.channel_attn._attn.2.weight', '_model.3._model.1.spatial_attn._model.0.weight', '_model.4._model.1.channel_attn._attn.0.weight', '_model.4._model.1.channel_attn._attn.2.weight', '_model.4._model.1.spatial_attn._model.0.weight', '_model.5._model.1.channel_attn._attn.0.weight', '_model.5._model.1.channel_attn._attn.2.weight', '_model.5._model.1.spatial_attn._model.0.weight', '_model.6._model.1.channel_attn._attn.0.weight', '_model.6._model.1.channel_attn._attn.2.weight', '_model.6._model.1.spatial_attn._model.0.weight'], unexpected_keys=['_model.0._model.3.weight', '_model.0._model.3.bias', '_model.0._model.4.channel_attn._attn.0.weight', '_model.0._model.4.channel_attn._attn.2.weight', '_model.0._model.4.spatial_attn._model.0.weight', '_model.0._model.1.weight', '_model.0._model.1.bias', '_model.0._model.1.running_mean', '_model.0._model.1.running_var', '_model.0._model.1.num_batches_tracked', '_model.1._model.3.weight', '_model.1._model.3.bias', '_model.1._model.4.channel_attn._attn.0.weight', '_model.1._model.4.channel_attn._attn.2.weight', '_model.1._model.4.spatial_attn._model.0.weight', '_model.1._model.1.weight', '_model.1._model.1.bias', '_model.1._model.1.running_mean', '_model.1._model.1.running_var', '_model.1._model.1.num_batches_tracked', '_model.2._model.3.weight', '_model.2._model.3.bias', '_model.2._model.4.channel_attn._attn.0.weight', '_model.2._model.4.channel_attn._attn.2.weight', '_model.2._model.4.spatial_attn._model.0.weight', '_model.2._model.1.weight', '_model.2._model.1.bias', '_model.2._model.1.running_mean', '_model.2._model.1.running_var', '_model.2._model.1.num_batches_tracked', '_model.3._model.3.weight', '_model.3._model.3.bias', '_model.3._model.4.channel_attn._attn.0.weight', '_model.3._model.4.channel_attn._attn.2.weight', '_model.3._model.4.spatial_attn._model.0.weight', '_model.3._model.1.weight', '_model.3._model.1.bias', '_model.3._model.1.running_mean', '_model.3._model.1.running_var', '_model.3._model.1.num_batches_tracked', '_model.4._model.3.weight', '_model.4._model.3.bias', '_model.4._model.4.channel_attn._attn.0.weight', '_model.4._model.4.channel_attn._attn.2.weight', '_model.4._model.4.spatial_attn._model.0.weight', '_model.4._model.1.weight', '_model.4._model.1.bias', '_model.4._model.1.running_mean', '_model.4._model.1.running_var', '_model.4._model.1.num_batches_tracked', '_model.5._model.3.weight', '_model.5._model.3.bias', '_model.5._model.4.channel_attn._attn.0.weight', '_model.5._model.4.channel_attn._attn.2.weight', '_model.5._model.4.spatial_attn._model.0.weight', '_model.5._model.1.weight', '_model.5._model.1.bias', '_model.5._model.1.running_mean', '_model.5._model.1.running_var', '_model.5._model.1.num_batches_tracked', '_model.6._model.3.weight', '_model.6._model.3.bias', '_model.6._model.4.channel_attn._attn.0.weight', '_model.6._model.4.channel_attn._attn.2.weight', '_model.6._model.4.spatial_attn._model.0.weight', '_model.6._model.1.weight', '_model.6._model.1.bias', '_model.6._model.1.running_mean', '_model.6._model.1.running_var', '_model.6._model.1.num_batches_tracked']))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(\n",
        "    model._cnn[0].load_state_dict(torch.load(\"/content/data_best_0_HR.pt\")[\"model\"] , strict=False ) ,\n",
        "    model._cnn[1].load_state_dict(torch.load(\"/content/data_best_0_OX.pt\")[\"model\"] , strict=False ) ,\n",
        "    model._cnn[2].load_state_dict(torch.load(\"/content/data_best_0_SaO2.pt\")[\"model\"] , strict=False ) ,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbV4NSKcgUUz",
        "outputId": "96e067a8-f8c6-4d92-c8fd-a7378c7fc5cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): EncoderCnnExtra(\n",
              "    (_model): Sequential(\n",
              "      (0): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=32, out_features=1, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=1, out_features=32, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=64, out_features=32, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=32, out_features=64, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(64, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(64, 128, kernel_size=(15,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=64, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=64, out_features=128, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(128, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(128, 256, kernel_size=(15,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=128, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=128, out_features=256, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(256, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(256, 512, kernel_size=(11,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=256, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=256, out_features=512, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(1024, 2048, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "          (3): Conv1d(2048, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (4): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(2048, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (7): AdaptiveAvgPool1d(output_size=1)\n",
              "    )\n",
              "  )\n",
              "  (1): EncoderCnnExtra(\n",
              "    (_model): Sequential(\n",
              "      (0): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (1): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (2): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(64, 128, kernel_size=(15,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (3): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(128, 256, kernel_size=(15,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (4): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(256, 512, kernel_size=(11,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (5): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (6): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(1024, 2048, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
              "        )\n",
              "        (_activation): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (7): AdaptiveAvgPool1d(output_size=1)\n",
              "    )\n",
              "  )\n",
              "  (2): EncoderCnnExtra(\n",
              "    (_model): Sequential(\n",
              "      (0): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=32, out_features=1, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=1, out_features=32, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=64, out_features=32, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=32, out_features=64, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(64, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(64, 128, kernel_size=(15,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=128, out_features=64, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=64, out_features=128, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(128, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(128, 256, kernel_size=(15,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=256, out_features=128, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=128, out_features=256, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(256, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(256, 512, kernel_size=(11,), stride=(2,), padding=(2,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=512, out_features=256, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=256, out_features=512, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BlockEncoderCnn(\n",
              "        (_model): Sequential(\n",
              "          (0): Conv1d(1024, 2048, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "          (1): DualAttention(\n",
              "            (channel_attn): ChannelAttention(\n",
              "              (_avgpool): AdaptiveAvgPool1d(output_size=1)\n",
              "              (_attn): Sequential(\n",
              "                (0): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "                (3): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "            (spatial_attn): SpatialAttentionBlock(\n",
              "              (_model): Sequential(\n",
              "                (0): Conv1d(2048, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "                (1): Sigmoid()\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (_activation): Sequential(\n",
              "          (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): ReLU()\n",
              "          (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (7): AdaptiveAvgPool1d(output_size=1)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model._cnn.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YiTxJH1ngW7_"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        inputs: logits با shape [batch_size, num_classes]\n",
        "        targets: اندیس کلاس صحیح با shape [batch_size]\n",
        "        \"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DSkrkZfhgY9t"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters() , lr=0.00001 ) # lr/epoch\n",
        "loss_func = FocalLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QB8w0q5gahd",
        "outputId": "39e53fe4-85e6-4bfa-90bc-4f8dbd9f2391"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.07it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 train_loss =>  0.7131936655325049 acc Train =>  tensor(0.3973) acc Test =>  tensor(0.4703)\n",
            "best acc test  tensor(0.4703)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.07it/s]\n",
            "100%|██████████| 17/17 [00:05<00:00,  2.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 train_loss =>  0.644093403044869 acc Train =>  tensor(0.4831) acc Test =>  tensor(0.6466)\n",
            "best acc test  tensor(0.6466)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:16<00:00,  4.22it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 train_loss =>  0.5746541487820008 acc Train =>  tensor(0.5998) acc Test =>  tensor(0.6931)\n",
            "best acc test  tensor(0.6931)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.06it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3 train_loss =>  0.52372186850099 acc Train =>  tensor(0.6512) acc Test =>  tensor(0.7489)\n",
            "best acc test  tensor(0.7489)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4 train_loss =>  0.47157972744282556 acc Train =>  tensor(0.7150) acc Test =>  tensor(0.7779)\n",
            "best acc test  tensor(0.7779)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.04it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5 train_loss =>  0.45351137659128976 acc Train =>  tensor(0.7325) acc Test =>  tensor(0.8303)\n",
            "best acc test  tensor(0.8303)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:14<00:00,  4.54it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6 train_loss =>  0.43413127169889565 acc Train =>  tensor(0.7491) acc Test =>  tensor(0.8465)\n",
            "best acc test  tensor(0.8465)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7 train_loss =>  0.42448671688051787 acc Train =>  tensor(0.7628) acc Test =>  tensor(0.8444)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:14<00:00,  4.85it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8 train_loss =>  0.41095390609082055 acc Train =>  tensor(0.7752) acc Test =>  tensor(0.8698)\n",
            "best acc test  tensor(0.8698)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.04it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9 train_loss =>  0.3910312135429943 acc Train =>  tensor(0.8016) acc Test =>  tensor(0.9097)\n",
            "best acc test  tensor(0.9097)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10 train_loss =>  0.3813592782791923 acc Train =>  tensor(0.8080) acc Test =>  tensor(0.8978)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.06it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 11 train_loss =>  0.3815799478222342 acc Train =>  tensor(0.8117) acc Test =>  tensor(0.9134)\n",
            "best acc test  tensor(0.9134)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 12 train_loss =>  0.3595693050061955 acc Train =>  tensor(0.8369) acc Test =>  tensor(0.9061)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.03it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13 train_loss =>  0.3644108382218024 acc Train =>  tensor(0.8271) acc Test =>  tensor(0.9281)\n",
            "best acc test  tensor(0.9281)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.04it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 14 train_loss =>  0.36348677777192173 acc Train =>  tensor(0.8325) acc Test =>  tensor(0.9059)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.06it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 15 train_loss =>  0.3508837759933051 acc Train =>  tensor(0.8485) acc Test =>  tensor(0.9167)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.07it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 16 train_loss =>  0.3560873164850123 acc Train =>  tensor(0.8369) acc Test =>  tensor(0.9257)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17 train_loss =>  0.35067971576662627 acc Train =>  tensor(0.8388) acc Test =>  tensor(0.8726)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.07it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 18 train_loss =>  0.3557818987790276 acc Train =>  tensor(0.8339) acc Test =>  tensor(0.9204)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 19 train_loss =>  0.3464997866574456 acc Train =>  tensor(0.8441) acc Test =>  tensor(0.9132)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 20 train_loss =>  0.3451533435898669 acc Train =>  tensor(0.8458) acc Test =>  tensor(0.9210)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.06it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 21 train_loss =>  0.34018261599190097 acc Train =>  tensor(0.8603) acc Test =>  tensor(0.9224)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 22 train_loss =>  0.3473982959985733 acc Train =>  tensor(0.8476) acc Test =>  tensor(0.9130)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 23 train_loss =>  0.3347255758502904 acc Train =>  tensor(0.8587) acc Test =>  tensor(0.9022)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.01it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 24 train_loss =>  0.3394908694659962 acc Train =>  tensor(0.8542) acc Test =>  tensor(0.9165)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.02it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 25 train_loss =>  0.3394211079267895 acc Train =>  tensor(0.8499) acc Test =>  tensor(0.9318)\n",
            "best acc test  tensor(0.9318)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.05it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 26 train_loss =>  0.33695056661963463 acc Train =>  tensor(0.8538) acc Test =>  tensor(0.8979)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:15<00:00,  4.50it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 27 train_loss =>  0.3433745665585293 acc Train =>  tensor(0.8470) acc Test =>  tensor(0.9204)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.07it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 28 train_loss =>  0.34949028294752627 acc Train =>  tensor(0.8367) acc Test =>  tensor(0.9071)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 68/68 [00:13<00:00,  5.04it/s]\n",
            "100%|██████████| 17/17 [00:03<00:00,  5.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 29 train_loss =>  0.34590739844476476 acc Train =>  tensor(0.8450) acc Test =>  tensor(0.9241)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "_lossed = []\n",
        "_acc_train = []\n",
        "_acc_test = []\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(30):\n",
        "\n",
        "    _list = []\n",
        "    acc = []\n",
        "    acc_train = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for data in tqdm(train_dataloder):\n",
        "\n",
        "        predictions = model(data[\"HR\"].to(device) , data[\"OX\"].to(device) , data[\"SaO2\"].to(device))\n",
        "\n",
        "        loss = loss_func(predictions, data[\"labels\"].squeeze(1).long().to(device))\n",
        "\n",
        "        predict_classes_val = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        acc_train.append(torch.sum(predict_classes_val.cpu() == data[\"labels\"].squeeze(1).long()) / predict_classes_val.size(0))\n",
        "\n",
        "        _list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        _ = nn.utils.clip_grad_norm_(model.parameters(), 50.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data in tqdm(valid_dataloder):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            predictions = model(data[\"HR\"].to(device) , data[\"OX\"].to(device) , data[\"SaO2\"].to(device))\n",
        "\n",
        "            predict_classes_val = torch.argmax(predictions, dim=1)\n",
        "\n",
        "        acc.append(torch.sum(predict_classes_val.cpu() == data[\"labels\"].squeeze(1).long()) / predict_classes_val.size(0))\n",
        "\n",
        "    print(\"epoch\", epoch , \"train_loss => \" , sum(_list) / len(_list) , \"acc Train => \" , sum(acc_train) / len(acc_train) , \"acc Test => \" , sum(acc) / len(acc))\n",
        "\n",
        "    if sum(acc) / len(acc) > best_acc:\n",
        "\n",
        "        torch.save({\n",
        "            \"model\": model.state_dict()\n",
        "        } , \"./data_best.pt\")\n",
        "\n",
        "        best_acc = sum(acc) / len(acc)\n",
        "\n",
        "        print(\"best acc test \" , best_acc)\n",
        "\n",
        "    _lossed.append(_list)\n",
        "    _acc_train.append(acc_train)\n",
        "    _acc_test.append(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LavHF34zgfW2",
        "outputId": "2e706fe6-5714-42fd-a4bd-6895d5a440f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 17/17 [00:03<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0     0.9489    0.9091    0.9286       143\n",
            "     Class 1     0.9015    0.9225    0.9119       129\n",
            "     Class 2     0.8831    0.9444    0.9128       144\n",
            "     Class 3     0.9746    0.9200    0.9465       125\n",
            "\n",
            "    accuracy                         0.9242       541\n",
            "   macro avg     0.9270    0.9240    0.9249       541\n",
            "weighted avg     0.9260    0.9242    0.9245       541\n",
            "\n",
            "\n",
            "Cohen's Kappa: 0.8988\n",
            "Average AUC: 0.9882\n",
            "Average AUPRC: 0.9739\n",
            "Average Macro-F1 (MF1): 0.9249\n",
            "Average Specificity (Spec): 0.9746\n",
            "\n",
            "Per-class metrics:\n",
            "Class 0: F1=0.929, Spec=0.982, AUC=0.990, AUPRC=0.979\n",
            "Class 1: F1=0.912, Spec=0.968, AUC=0.984, AUPRC=0.963\n",
            "Class 2: F1=0.913, Spec=0.955, AUC=0.988, AUPRC=0.975\n",
            "Class 3: F1=0.947, Spec=0.993, AUC=0.990, AUPRC=0.979\n",
            "\n",
            "✅ All figures saved to: /content/results_plots\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    cohen_kappa_score,\n",
        "    f1_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# ==== Configuration ====\n",
        "save_dir = \"results_plots\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# --- Data collection ---\n",
        "all_real = []\n",
        "all_predict = []\n",
        "all_probs = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(valid_dataloder):\n",
        "        inputs = [data[\"HR\"].to(device), data[\"OX\"].to(device), data[\"SaO2\"].to(device)]\n",
        "        labels = data[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(*inputs)  # logits\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        all_real.append(labels.cpu())\n",
        "        all_predict.append(preds.cpu())\n",
        "        all_probs.append(probs.cpu())\n",
        "\n",
        "# --- Convert to numpy arrays ---\n",
        "all_real = torch.cat(all_real).numpy()\n",
        "all_predict = torch.cat(all_predict).numpy()\n",
        "all_probs = torch.cat(all_probs).numpy()\n",
        "\n",
        "num_classes = all_probs.shape[1]\n",
        "class_names = [f\"Class {i}\" for i in range(num_classes)]  # update if you have real names\n",
        "\n",
        "# --- 1️⃣ Classification report ---\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(all_real, all_predict, target_names=class_names, digits=4))\n",
        "\n",
        "# --- 2️⃣ Confusion Matrix ---\n",
        "cm = confusion_matrix(all_real, all_predict)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --- 3️⃣ Extra metrics ---\n",
        "kappa = cohen_kappa_score(all_real, all_predict)\n",
        "\n",
        "# ✅ Specificity per class\n",
        "specificity = []\n",
        "for i in range(num_classes):\n",
        "    tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "    fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    specificity.append(spec)\n",
        "\n",
        "avg_spec = np.mean(specificity)  # Average specificity (macro)\n",
        "\n",
        "# --- 4️⃣ F1 per class and MF1 ---\n",
        "f1_per_class = f1_score(all_real, all_predict, average=None)\n",
        "avg_mf1 = f1_score(all_real, all_predict, average='macro')\n",
        "\n",
        "# --- 5️⃣ ROC & PR Curves ---\n",
        "auc_per_class = []\n",
        "auprc_per_class = []\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "for i, cname in enumerate(class_names):\n",
        "    y_true = (all_real == i).astype(int)\n",
        "    y_score = all_probs[:, i]\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = roc_auc_score(y_true, y_score)\n",
        "    auc_per_class.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, label=f\"{cname} (AUC={roc_auc:.3f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "plt.title(\"ROC Curves (All Classes)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_dir, \"roc_curves_all_classes.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --- 6️⃣ AUPRC Curves ---\n",
        "plt.figure(figsize=(7, 6))\n",
        "for i, cname in enumerate(class_names):\n",
        "    y_true = (all_real == i).astype(int)\n",
        "    y_score = all_probs[:, i]\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    auprc_per_class.append(pr_auc)\n",
        "    plt.plot(recall, precision, label=f\"{cname} (AUPRC={pr_auc:.3f})\")\n",
        "\n",
        "plt.title(\"Precision-Recall Curves (All Classes)\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_dir, \"precision_recall_curves_all_classes.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --- 7️⃣ Print Summary ---\n",
        "avg_auc = np.mean(auc_per_class)\n",
        "avg_auprc = np.mean(auprc_per_class)\n",
        "\n",
        "print(f\"\\nCohen's Kappa: {kappa:.4f}\")\n",
        "print(f\"Average AUC: {avg_auc:.4f}\")\n",
        "print(f\"Average AUPRC: {avg_auprc:.4f}\")\n",
        "print(f\"Average Macro-F1 (MF1): {avg_mf1:.4f}\")\n",
        "print(f\"Average Specificity (Spec): {avg_spec:.4f}\\n\")\n",
        "\n",
        "print(\"Per-class metrics:\")\n",
        "for i, cname in enumerate(class_names):\n",
        "    print(f\"{cname}: F1={f1_per_class[i]:.3f}, Spec={specificity[i]:.3f}, AUC={auc_per_class[i]:.3f}, AUPRC={auprc_per_class[i]:.3f}\")\n",
        "\n",
        "print(f\"\\n✅ All figures saved to: {os.path.abspath(save_dir)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
